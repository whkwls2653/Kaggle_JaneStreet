{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_JaneStreet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOt25YIZY6TSrYbQhEj4u+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkwls2653/Kaggle_JaneStreet/blob/main/Kaggle_JaneStreet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImpWZTMBWOFp"
      },
      "source": [
        "#  환경설정 kaggle API & Google cloud mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDc6vbLlEpy"
      },
      "source": [
        "#kaggle  api 설치\r\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm0ztXENML3h",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "720b7a3d-ed7c-488f-b5ec-085dd6c92274"
      },
      "source": [
        "#kaggle에서 kaggle.json file 다운로드후 아래 코드 실행, 아래 파일선택 누른후 kaggl.json file 업로드\r\n",
        "#kaggle.json file 다운받는법은 주소 참조 https://kaggle-kr.tistory.com/21\r\n",
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5d922dc0-5ff4-4113-9cb6-fb1d3f8fa005\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5d922dc0-5ff4-4113-9cb6-fb1d3f8fa005\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jwajinlee\",\"key\":\"c39b074a7d40782121b24bc84667a68a\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idqp2DRSj1f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3a8a9d-4820-433a-a26e-e08fd80e8508"
      },
      "source": [
        "#kaggle 폴더 만든후 안에 업로드한 kaggle.json 넣어줌\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "# Permission Warning 이 일어나지 않도록 \r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "# 본인이 참가한 모든 대회 보기 \r\n",
        "!kaggle competitions list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started     Prizes        107           False  \n",
            "gan-getting-started                            2030-07-01 23:59:00  Getting Started     Prizes        215           False  \n",
            "tpu-getting-started                            2030-06-03 23:59:00  Getting Started  Knowledge        422           False  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2895           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge      22337           False  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       5982           False  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started  Knowledge        524           False  \n",
            "nlp-getting-started                            2030-01-01 00:00:00  Getting Started  Knowledge       1559           False  \n",
            "competitive-data-science-predict-future-sales  2022-12-31 23:59:00  Playground           Kudos      10174           False  \n",
            "vinbigdata-chest-xray-abnormalities-detection  2021-03-30 23:59:00  Featured           $50,000        343           False  \n",
            "hubmap-kidney-segmentation                     2021-03-25 23:59:00  Research           $60,000        891           False  \n",
            "ranzcr-clip-catheter-line-classification       2021-03-15 23:59:00  Featured           $50,000        560           False  \n",
            "jane-street-market-prediction                  2021-02-22 23:59:00  Featured          $100,000       2804            True  \n",
            "cassava-leaf-disease-classification            2021-02-18 23:59:00  Research           $18,000       2924           False  \n",
            "rfcx-species-audio-detection                   2021-02-17 23:59:00  Research           $15,000        879           False  \n",
            "acea-water-prediction                          2021-02-17 23:59:00  Analytics          $25,000          0           False  \n",
            "rock-paper-scissors                            2021-02-01 23:59:00  Playground          Prizes       1518           False  \n",
            "santa-2020                                     2021-02-01 23:59:00  Featured            Prizes        761           False  \n",
            "tabular-playground-series-jan-2021             2021-01-31 23:59:00  Playground            Swag       1215           False  \n",
            "nfl-big-data-bowl-2021                         2021-01-07 23:59:00  Analytics         $100,000          0           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLFjb5s8blWx",
        "outputId": "6c7c6177-e999-4453-c507-482151e0a5aa"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7nslHJglDDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17df9eb0-bf50-49d7-c22f-86cf4f8d4dab"
      },
      "source": [
        "#jane-street-market prediction dataset 다운로드\r\n",
        "! kaggle competitions download -c jane-street-market-prediction \r\n",
        "## 하고나면 왼쪽  폴더모양 클릭하면 content/안에 example_sample..., example_test.csv.zip, features.csv,  train.csv.zip 이 생겼을 겁니다."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading __init__.py to /content\n",
            "  0% 0.00/59.0 [00:00<?, ?B/s]\n",
            "100% 59.0/59.0 [00:00<00:00, 126kB/s]\n",
            "Downloading competition.cpython-37m-x86_64-linux-gnu.so to /content\n",
            "  0% 0.00/441k [00:00<?, ?B/s]\n",
            "100% 441k/441k [00:00<00:00, 61.1MB/s]\n",
            "Downloading example_test.csv.zip to /content\n",
            " 55% 9.00M/16.4M [00:00<00:00, 60.1MB/s]\n",
            "100% 16.4M/16.4M [00:00<00:00, 79.9MB/s]\n",
            "Downloading features.csv to /content\n",
            "  0% 0.00/23.3k [00:00<?, ?B/s]\n",
            "100% 23.3k/23.3k [00:00<00:00, 24.2MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "100% 2.61G/2.61G [00:29<00:00, 88.4MB/s]\n",
            "100% 2.61G/2.61G [00:29<00:00, 94.5MB/s]\n",
            "Downloading example_sample_submission.csv to /content\n",
            "  0% 0.00/108k [00:00<?, ?B/s]\n",
            "100% 108k/108k [00:00<00:00, 89.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bcD2rwbBQ7",
        "outputId": "894cc434-b92d-4f4e-e5e1-6119827ed2ad"
      },
      "source": [
        "#gdrive 마운트 하기. 코드실행후 아래 나오는 안내대로 따라서 인증\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZvoqD2JRfTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d399e5c-076b-4451-b42f-cc64f56f5077"
      },
      "source": [
        "#G드라이브 안에 kaggle 폴더만들기\r\n",
        "!mkdir /content/gdrive/MyDrive/Kaggle\r\n",
        "#gdrive 안에 방금만든 kaggle폴더로 kaggle dataset 욺기기\r\n",
        "!cp /content/*.zip /content/gdrive/MyDrive/Kaggle/\r\n",
        "!cp /content/*.csv /content/gdrive/MyDrive/Kaggle/\r\n",
        "\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/Kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjBdFl0VUt4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e42888-a6cd-4d63-fff3-478875f93f2c"
      },
      "source": [
        "#zip file 압축풀기\r\n",
        "!unzip /content/gdrive/MyDrive/Kaggle/example_test.csv.zip -d /content/gdrive/MyDrive/Kaggle/\r\n",
        "!unzip /content/gdrive/MyDrive/Kaggle/train.csv.zip -d /content/gdrive/MyDrive/Kaggle/\r\n",
        "#######################여기까지 하면 댁의 gdrive안에 Kaggle폴더에 모든 파일이 다운로드 되어 사용가능한 형태입니다.\r\n",
        "#이제 다시 연결할때마다 위에 했던 gdrive 마운트를 한후(아래 세줄은 gdrive 마운트 하는방법 반복서술 해놓은것)\r\n",
        "\r\n",
        "#gdrive 마운트 하기. 코드실행후 아래 나오는 안내대로 따라서 인증\r\n",
        "#from google.colab import drive\r\n",
        "#drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "#참조할때는 = /content/gdrive/MyDrive/Kaggle/파일명  으로 사용하면 됩니다.\r\n",
        "# ex.train.csv 불러오기 ->     train = pd.read_csv('/content/gdrive/MyDrive/Kaggle/train.csv')\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/Kaggle/example_test.csv.zip\n",
            "  inflating: /content/gdrive/MyDrive/Kaggle/example_test.csv  \n",
            "Archive:  /content/gdrive/MyDrive/Kaggle/train.csv.zip\n",
            "  inflating: /content/gdrive/MyDrive/Kaggle/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUDzZo5LWGyh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTcqKNbXWJd7"
      },
      "source": [
        "# python 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sZFwM57WHNZ"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "for dirname, _, filenames in os.walk('/content/gdrive/MyDrive/Kaggle/JaneStreet'):\r\n",
        "  for filename in filenames:\r\n",
        "    print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxdt0-VFYBNX",
        "outputId": "baffd852-9bf0-4bf1-a708-1dc74d67398c"
      },
      "source": [
        "!pip install git+https://github.com/keras-team/keras-tuner.git\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/keras-team/keras-tuner.git\n",
            "  Cloning https://github.com/keras-team/keras-tuner.git to /tmp/pip-req-build-0n6tf9dg\n",
            "  Running command git clone -q https://github.com/keras-team/keras-tuner.git /tmp/pip-req-build-0n6tf9dg\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (20.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (2.4.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from keras-tuner==1.0.3) (5.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner==1.0.3) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (51.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.36.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->keras-tuner==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (4.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->keras-tuner==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->keras-tuner==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner==1.0.3) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->keras-tuner==1.0.3) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner==1.0.3) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner==1.0.3) (0.4.8)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.3-cp36-none-any.whl size=92461 sha256=43f6f38902e63ce014f9e84e6a43966628f5b6e3feac0247c66bea05259ca0a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5aownk3s/wheels/33/f9/be/250538026c0381bfee3fe34b533f483c15dbfde8fbb61c8bbd\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=329835cac0c7dad6a21750e1e30d60232126f45de20c9c8289aa6997f7c75138\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.3 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4URIvPELXyZB"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import GroupKFold\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "from random import choices\r\n",
        "\r\n",
        "\r\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azo-v2dqn4Oq"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\r\n",
        "from sklearn.utils.validation import _deprecate_positional_args\r\n",
        "\r\n",
        "# modified code for group gaps; source\r\n",
        "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\r\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\r\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\r\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\r\n",
        "    train into test if the model has windowed or lag features.\r\n",
        "    Provides train/test indices to split time series data samples\r\n",
        "    that are observed at fixed time intervals according to a\r\n",
        "    third-party provided group.\r\n",
        "    In each split, test indices must be higher than before, and thus shuffling\r\n",
        "    in cross validator is inappropriate.\r\n",
        "    This cross-validation object is a variation of :class:`KFold`.\r\n",
        "    In the kth split, it returns first k folds as train set and the\r\n",
        "    (k+1)th fold as test set.\r\n",
        "    The same group will not appear in two different folds (the number of\r\n",
        "    distinct groups has to be at least equal to the number of folds).\r\n",
        "    Note that unlike standard cross-validation methods, successive\r\n",
        "    training sets are supersets of those that come before them.\r\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    n_splits : int, default=5\r\n",
        "        Number of splits. Must be at least 2.\r\n",
        "    max_train_group_size : int, default=Inf\r\n",
        "        Maximum group size for a single training set.\r\n",
        "    group_gap : int, default=None\r\n",
        "        Gap between train and test\r\n",
        "    max_test_group_size : int, default=Inf\r\n",
        "        We discard this number of groups from the end of each train split\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    @_deprecate_positional_args\r\n",
        "    def __init__(self,\r\n",
        "                 n_splits=5,\r\n",
        "                 *,\r\n",
        "                 max_train_group_size=np.inf,\r\n",
        "                 max_test_group_size=np.inf,\r\n",
        "                 group_gap=None,\r\n",
        "                 verbose=False\r\n",
        "                 ):\r\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\r\n",
        "        self.max_train_group_size = max_train_group_size\r\n",
        "        self.group_gap = group_gap\r\n",
        "        self.max_test_group_size = max_test_group_size\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "    def split(self, X, y=None, groups=None):\r\n",
        "        \"\"\"Generate indices to split data into training and test set.\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        X : array-like of shape (n_samples, n_features)\r\n",
        "            Training data, where n_samples is the number of samples\r\n",
        "            and n_features is the number of features.\r\n",
        "        y : array-like of shape (n_samples,)\r\n",
        "            Always ignored, exists for compatibility.\r\n",
        "        groups : array-like of shape (n_samples,)\r\n",
        "            Group labels for the samples used while splitting the dataset into\r\n",
        "            train/test set.\r\n",
        "        Yields\r\n",
        "        ------\r\n",
        "        train : ndarray\r\n",
        "            The training set indices for that split.\r\n",
        "        test : ndarray\r\n",
        "            The testing set indices for that split.\r\n",
        "        \"\"\"\r\n",
        "        if groups is None:\r\n",
        "            raise ValueError(\r\n",
        "                \"The 'groups' parameter should not be None\")\r\n",
        "        X, y, groups = indexable(X, y, groups)\r\n",
        "        n_samples = _num_samples(X)\r\n",
        "        n_splits = self.n_splits\r\n",
        "        group_gap = self.group_gap\r\n",
        "        max_test_group_size = self.max_test_group_size\r\n",
        "        max_train_group_size = self.max_train_group_size\r\n",
        "        n_folds = n_splits + 1\r\n",
        "        group_dict = {}\r\n",
        "        u, ind = np.unique(groups, return_index=True)\r\n",
        "        unique_groups = u[np.argsort(ind)]\r\n",
        "        n_samples = _num_samples(X)\r\n",
        "        n_groups = _num_samples(unique_groups)\r\n",
        "        for idx in np.arange(n_samples):\r\n",
        "            if (groups[idx] in group_dict):\r\n",
        "                group_dict[groups[idx]].append(idx)\r\n",
        "            else:\r\n",
        "                group_dict[groups[idx]] = [idx]\r\n",
        "        if n_folds > n_groups:\r\n",
        "            raise ValueError(\r\n",
        "                (\"Cannot have number of folds={0} greater than\"\r\n",
        "                 \" the number of groups={1}\").format(n_folds,\r\n",
        "                                                     n_groups))\r\n",
        "\r\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\r\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\r\n",
        "                                  n_groups, group_test_size)\r\n",
        "        for group_test_start in group_test_starts:\r\n",
        "            train_array = []\r\n",
        "            test_array = []\r\n",
        "\r\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\r\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\r\n",
        "                train_array_tmp = group_dict[train_group_idx]\r\n",
        "                \r\n",
        "                train_array = np.sort(np.unique(\r\n",
        "                                      np.concatenate((train_array,\r\n",
        "                                                      train_array_tmp)),\r\n",
        "                                      axis=None), axis=None)\r\n",
        "\r\n",
        "            train_end = train_array.size\r\n",
        " \r\n",
        "            for test_group_idx in unique_groups[group_test_start:\r\n",
        "                                                group_test_start +\r\n",
        "                                                group_test_size]:\r\n",
        "                test_array_tmp = group_dict[test_group_idx]\r\n",
        "                test_array = np.sort(np.unique(\r\n",
        "                                              np.concatenate((test_array,\r\n",
        "                                                              test_array_tmp)),\r\n",
        "                                     axis=None), axis=None)\r\n",
        "\r\n",
        "            test_array  = test_array[group_gap:]\r\n",
        "            \r\n",
        "            \r\n",
        "            if self.verbose > 0:\r\n",
        "                    pass\r\n",
        "                    \r\n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxguV8DQn4tR"
      },
      "source": [
        "class CVTuner(kt.engine.tuner.Tuner):\r\n",
        "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\r\n",
        "        val_losses = []\r\n",
        "        for train_indices, test_indices in splits:\r\n",
        "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\r\n",
        "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\r\n",
        "            if len(X_train) < 2:\r\n",
        "                X_train = X_train[0]\r\n",
        "                X_test = X_test[0]\r\n",
        "            if len(y_train) < 2:\r\n",
        "                y_train = y_train[0]\r\n",
        "                y_test = y_test[0]\r\n",
        "            \r\n",
        "            model = self.hypermodel.build(trial.hyperparameters)\r\n",
        "            hist = model.fit(X_train,y_train,\r\n",
        "                      validation_data=(X_test,y_test),\r\n",
        "                      epochs=epochs,\r\n",
        "                        batch_size=batch_size,\r\n",
        "                      callbacks=callbacks)\r\n",
        "            \r\n",
        "            val_losses.append([hist.history[k][-1] for k in hist.history])\r\n",
        "        val_losses = np.asarray(val_losses)\r\n",
        "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\r\n",
        "        self.save_model(trial.trial_id, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K88LHvrkn40h"
      },
      "source": [
        "TRAINING = False\r\n",
        "USE_FINETUNE = False     \r\n",
        "FOLDS = 5\r\n",
        "SEED = 42\r\n",
        "\r\n",
        "train = pd.read_csv('/content/gdrive/MyDrive/Kaggle/janestreet/train.csv')\r\n",
        "train = train.query('date > 85').reset_index(drop = True) \r\n",
        "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\r\n",
        "train.fillna(train.mean(),inplace=True)\r\n",
        "train = train.query('weight > 0').reset_index(drop = True)\r\n",
        "#train['action'] = (train['resp'] > 0).astype('int')\r\n",
        "train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\r\n",
        "features = [c for c in train.columns if 'feature' in c]\r\n",
        "\r\n",
        "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\r\n",
        "\r\n",
        "X = train[features].values\r\n",
        "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\r\n",
        "\r\n",
        "f_mean = np.mean(train[features[1:]].values,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcKyXhfcqPie"
      },
      "source": [
        "def create_autoencoder(input_dim,output_dim,noise=0.05):\r\n",
        "    i = Input(input_dim)\r\n",
        "    encoded = BatchNormalization()(i)\r\n",
        "    encoded = GaussianNoise(noise)(encoded)\r\n",
        "    encoded = Dense(64,activation='relu')(encoded)\r\n",
        "    decoded = Dropout(0.2)(encoded)\r\n",
        "    decoded = Dense(input_dim,name='decoded')(decoded)\r\n",
        "    x = Dense(32,activation='relu')(decoded)\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(0.2)(x)\r\n",
        "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\r\n",
        "    \r\n",
        "    encoder = Model(inputs=i,outputs=encoded)\r\n",
        "    autoencoder = Model(inputs=i,outputs=[decoded,x])\r\n",
        "    \r\n",
        "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\r\n",
        "    return autoencoder, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml2xOFKxqPgH"
      },
      "source": [
        "def create_model(hp,input_dim,output_dim,encoder):\r\n",
        "    inputs = Input(input_dim)\r\n",
        "    \r\n",
        "    x = encoder(inputs)\r\n",
        "    x = Concatenate()([x,inputs]) #use both raw and encoded features\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\r\n",
        "    \r\n",
        "    for i in range(hp.Int('num_layers',1,3)):\r\n",
        "        x = Dense(hp.Int('num_units_{i}',64,256))(x)\r\n",
        "        x = BatchNormalization()(x)\r\n",
        "        x = Lambda(tf.keras.activations.swish)(x)\r\n",
        "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\r\n",
        "    x = Dense(output_dim,activation='sigmoid')(x)\r\n",
        "    model = Model(inputs=inputs,outputs=x)\r\n",
        "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvVA-wIqPdf",
        "outputId": "f96ae6f0-8142-43b2-89c5-c499a3aa34b3"
      },
      "source": [
        "TRAINING=True\r\n",
        "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\r\n",
        "if TRAINING:\r\n",
        "    autoencoder.fit(X,(X,y),\r\n",
        "                    epochs=1000,\r\n",
        "                    batch_size=4096, \r\n",
        "                    validation_split=0.1,\r\n",
        "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\r\n",
        "    encoder.save_weights('/content/gdrive/MyDrive/Kaggle/janestreet/encoder.hdf5')\r\n",
        "# else:\r\n",
        "    # encoder.load_weights('../input/vae223/encoder.hdf5')\r\n",
        "encoder.trainable = False\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "346/346 [==============================] - 17s 45ms/step - loss: 3.7339 - decoded_loss: 2.9908 - label_output_loss: 0.7431 - val_loss: 1.3724 - val_decoded_loss: 0.6811 - val_label_output_loss: 0.6913\n",
            "Epoch 2/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.8848 - decoded_loss: 1.1880 - label_output_loss: 0.6968 - val_loss: 1.2357 - val_decoded_loss: 0.5454 - val_label_output_loss: 0.6904\n",
            "Epoch 3/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.8454 - decoded_loss: 1.1532 - label_output_loss: 0.6922 - val_loss: 1.1746 - val_decoded_loss: 0.4848 - val_label_output_loss: 0.6898\n",
            "Epoch 4/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.7441 - decoded_loss: 1.0529 - label_output_loss: 0.6912 - val_loss: 1.1413 - val_decoded_loss: 0.4516 - val_label_output_loss: 0.6897\n",
            "Epoch 5/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.7466 - decoded_loss: 1.0555 - label_output_loss: 0.6911 - val_loss: 1.1235 - val_decoded_loss: 0.4340 - val_label_output_loss: 0.6895\n",
            "Epoch 6/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.7141 - decoded_loss: 1.0232 - label_output_loss: 0.6909 - val_loss: 1.1134 - val_decoded_loss: 0.4239 - val_label_output_loss: 0.6894\n",
            "Epoch 7/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.7248 - decoded_loss: 1.0340 - label_output_loss: 0.6908 - val_loss: 1.0978 - val_decoded_loss: 0.4085 - val_label_output_loss: 0.6893\n",
            "Epoch 8/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.7416 - decoded_loss: 1.0509 - label_output_loss: 0.6907 - val_loss: 1.0921 - val_decoded_loss: 0.4029 - val_label_output_loss: 0.6893\n",
            "Epoch 9/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6727 - decoded_loss: 0.9820 - label_output_loss: 0.6907 - val_loss: 1.0869 - val_decoded_loss: 0.3978 - val_label_output_loss: 0.6891\n",
            "Epoch 10/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6610 - decoded_loss: 0.9704 - label_output_loss: 0.6906 - val_loss: 1.0738 - val_decoded_loss: 0.3848 - val_label_output_loss: 0.6889\n",
            "Epoch 11/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6506 - decoded_loss: 0.9600 - label_output_loss: 0.6906 - val_loss: 1.0790 - val_decoded_loss: 0.3900 - val_label_output_loss: 0.6890\n",
            "Epoch 12/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6668 - decoded_loss: 0.9763 - label_output_loss: 0.6906 - val_loss: 1.0591 - val_decoded_loss: 0.3701 - val_label_output_loss: 0.6890\n",
            "Epoch 13/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.7000 - decoded_loss: 1.0096 - label_output_loss: 0.6904 - val_loss: 1.0719 - val_decoded_loss: 0.3828 - val_label_output_loss: 0.6891\n",
            "Epoch 14/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6477 - decoded_loss: 0.9573 - label_output_loss: 0.6904 - val_loss: 1.0615 - val_decoded_loss: 0.3726 - val_label_output_loss: 0.6890\n",
            "Epoch 15/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6333 - decoded_loss: 0.9429 - label_output_loss: 0.6904 - val_loss: 1.0585 - val_decoded_loss: 0.3696 - val_label_output_loss: 0.6889\n",
            "Epoch 16/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6522 - decoded_loss: 0.9618 - label_output_loss: 0.6904 - val_loss: 1.0593 - val_decoded_loss: 0.3703 - val_label_output_loss: 0.6890\n",
            "Epoch 17/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6948 - decoded_loss: 1.0045 - label_output_loss: 0.6904 - val_loss: 1.0507 - val_decoded_loss: 0.3617 - val_label_output_loss: 0.6890\n",
            "Epoch 18/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6315 - decoded_loss: 0.9411 - label_output_loss: 0.6903 - val_loss: 1.0448 - val_decoded_loss: 0.3561 - val_label_output_loss: 0.6887\n",
            "Epoch 19/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6344 - decoded_loss: 0.9441 - label_output_loss: 0.6902 - val_loss: 1.0467 - val_decoded_loss: 0.3579 - val_label_output_loss: 0.6888\n",
            "Epoch 20/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6353 - decoded_loss: 0.9451 - label_output_loss: 0.6903 - val_loss: 1.0506 - val_decoded_loss: 0.3617 - val_label_output_loss: 0.6888\n",
            "Epoch 21/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6524 - decoded_loss: 0.9622 - label_output_loss: 0.6902 - val_loss: 1.0356 - val_decoded_loss: 0.3467 - val_label_output_loss: 0.6890\n",
            "Epoch 22/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6459 - decoded_loss: 0.9556 - label_output_loss: 0.6903 - val_loss: 1.0382 - val_decoded_loss: 0.3494 - val_label_output_loss: 0.6888\n",
            "Epoch 23/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6579 - decoded_loss: 0.9678 - label_output_loss: 0.6901 - val_loss: 1.0403 - val_decoded_loss: 0.3515 - val_label_output_loss: 0.6888\n",
            "Epoch 24/1000\n",
            "346/346 [==============================] - 16s 47ms/step - loss: 1.6207 - decoded_loss: 0.9306 - label_output_loss: 0.6901 - val_loss: 1.0385 - val_decoded_loss: 0.3499 - val_label_output_loss: 0.6886\n",
            "Epoch 25/1000\n",
            "346/346 [==============================] - 16s 46ms/step - loss: 1.6654 - decoded_loss: 0.9753 - label_output_loss: 0.6901 - val_loss: 1.0377 - val_decoded_loss: 0.3489 - val_label_output_loss: 0.6888\n",
            "Epoch 26/1000\n",
            "346/346 [==============================] - 16s 47ms/step - loss: 1.6004 - decoded_loss: 0.9103 - label_output_loss: 0.6901 - val_loss: 1.0443 - val_decoded_loss: 0.3556 - val_label_output_loss: 0.6887\n",
            "Epoch 27/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6111 - decoded_loss: 0.9210 - label_output_loss: 0.6901 - val_loss: 1.0431 - val_decoded_loss: 0.3543 - val_label_output_loss: 0.6888\n",
            "Epoch 28/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6069 - decoded_loss: 0.9169 - label_output_loss: 0.6900 - val_loss: 1.0364 - val_decoded_loss: 0.3476 - val_label_output_loss: 0.6888\n",
            "Epoch 29/1000\n",
            "346/346 [==============================] - 16s 46ms/step - loss: 1.6084 - decoded_loss: 0.9184 - label_output_loss: 0.6900 - val_loss: 1.0360 - val_decoded_loss: 0.3473 - val_label_output_loss: 0.6886\n",
            "Epoch 30/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6402 - decoded_loss: 0.9502 - label_output_loss: 0.6900 - val_loss: 1.0348 - val_decoded_loss: 0.3461 - val_label_output_loss: 0.6887\n",
            "Epoch 31/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6074 - decoded_loss: 0.9174 - label_output_loss: 0.6900 - val_loss: 1.0349 - val_decoded_loss: 0.3463 - val_label_output_loss: 0.6887\n",
            "Epoch 32/1000\n",
            "346/346 [==============================] - 16s 46ms/step - loss: 1.5975 - decoded_loss: 0.9075 - label_output_loss: 0.6900 - val_loss: 1.0295 - val_decoded_loss: 0.3411 - val_label_output_loss: 0.6884\n",
            "Epoch 33/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6129 - decoded_loss: 0.9231 - label_output_loss: 0.6899 - val_loss: 1.0246 - val_decoded_loss: 0.3360 - val_label_output_loss: 0.6886\n",
            "Epoch 34/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6501 - decoded_loss: 0.9602 - label_output_loss: 0.6899 - val_loss: 1.0286 - val_decoded_loss: 0.3400 - val_label_output_loss: 0.6887\n",
            "Epoch 35/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6312 - decoded_loss: 0.9412 - label_output_loss: 0.6900 - val_loss: 1.0300 - val_decoded_loss: 0.3414 - val_label_output_loss: 0.6886\n",
            "Epoch 36/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6181 - decoded_loss: 0.9282 - label_output_loss: 0.6899 - val_loss: 1.0356 - val_decoded_loss: 0.3470 - val_label_output_loss: 0.6886\n",
            "Epoch 37/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6053 - decoded_loss: 0.9154 - label_output_loss: 0.6899 - val_loss: 1.0256 - val_decoded_loss: 0.3370 - val_label_output_loss: 0.6887\n",
            "Epoch 38/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.5869 - decoded_loss: 0.8970 - label_output_loss: 0.6899 - val_loss: 1.0318 - val_decoded_loss: 0.3431 - val_label_output_loss: 0.6887\n",
            "Epoch 39/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6076 - decoded_loss: 0.9176 - label_output_loss: 0.6900 - val_loss: 1.0295 - val_decoded_loss: 0.3410 - val_label_output_loss: 0.6886\n",
            "Epoch 40/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6126 - decoded_loss: 0.9226 - label_output_loss: 0.6900 - val_loss: 1.0273 - val_decoded_loss: 0.3387 - val_label_output_loss: 0.6886\n",
            "Epoch 41/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5954 - decoded_loss: 0.9054 - label_output_loss: 0.6901 - val_loss: 1.0330 - val_decoded_loss: 0.3443 - val_label_output_loss: 0.6887\n",
            "Epoch 42/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.5785 - decoded_loss: 0.8887 - label_output_loss: 0.6899 - val_loss: 1.0254 - val_decoded_loss: 0.3367 - val_label_output_loss: 0.6887\n",
            "Epoch 43/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6058 - decoded_loss: 0.9158 - label_output_loss: 0.6900 - val_loss: 1.0239 - val_decoded_loss: 0.3353 - val_label_output_loss: 0.6886\n",
            "Epoch 44/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5928 - decoded_loss: 0.9030 - label_output_loss: 0.6899 - val_loss: 1.0221 - val_decoded_loss: 0.3335 - val_label_output_loss: 0.6886\n",
            "Epoch 45/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6477 - decoded_loss: 0.9577 - label_output_loss: 0.6899 - val_loss: 1.0202 - val_decoded_loss: 0.3316 - val_label_output_loss: 0.6886\n",
            "Epoch 46/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6692 - decoded_loss: 0.9793 - label_output_loss: 0.6899 - val_loss: 1.0217 - val_decoded_loss: 0.3332 - val_label_output_loss: 0.6885\n",
            "Epoch 47/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6016 - decoded_loss: 0.9117 - label_output_loss: 0.6898 - val_loss: 1.0242 - val_decoded_loss: 0.3355 - val_label_output_loss: 0.6886\n",
            "Epoch 48/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.5910 - decoded_loss: 0.9013 - label_output_loss: 0.6897 - val_loss: 1.0211 - val_decoded_loss: 0.3325 - val_label_output_loss: 0.6887\n",
            "Epoch 49/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6139 - decoded_loss: 0.9241 - label_output_loss: 0.6898 - val_loss: 1.0270 - val_decoded_loss: 0.3385 - val_label_output_loss: 0.6885\n",
            "Epoch 50/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6205 - decoded_loss: 0.9307 - label_output_loss: 0.6897 - val_loss: 1.0226 - val_decoded_loss: 0.3340 - val_label_output_loss: 0.6886\n",
            "Epoch 51/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6197 - decoded_loss: 0.9298 - label_output_loss: 0.6898 - val_loss: 1.0202 - val_decoded_loss: 0.3317 - val_label_output_loss: 0.6885\n",
            "Epoch 52/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6025 - decoded_loss: 0.9127 - label_output_loss: 0.6898 - val_loss: 1.0190 - val_decoded_loss: 0.3304 - val_label_output_loss: 0.6886\n",
            "Epoch 53/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6275 - decoded_loss: 0.9377 - label_output_loss: 0.6898 - val_loss: 1.0176 - val_decoded_loss: 0.3289 - val_label_output_loss: 0.6887\n",
            "Epoch 54/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6043 - decoded_loss: 0.9145 - label_output_loss: 0.6897 - val_loss: 1.0240 - val_decoded_loss: 0.3354 - val_label_output_loss: 0.6886\n",
            "Epoch 55/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6003 - decoded_loss: 0.9105 - label_output_loss: 0.6898 - val_loss: 1.0246 - val_decoded_loss: 0.3359 - val_label_output_loss: 0.6887\n",
            "Epoch 56/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6118 - decoded_loss: 0.9219 - label_output_loss: 0.6898 - val_loss: 1.0236 - val_decoded_loss: 0.3350 - val_label_output_loss: 0.6886\n",
            "Epoch 57/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5989 - decoded_loss: 0.9091 - label_output_loss: 0.6898 - val_loss: 1.0184 - val_decoded_loss: 0.3297 - val_label_output_loss: 0.6887\n",
            "Epoch 58/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6110 - decoded_loss: 0.9212 - label_output_loss: 0.6898 - val_loss: 1.0203 - val_decoded_loss: 0.3317 - val_label_output_loss: 0.6886\n",
            "Epoch 59/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6363 - decoded_loss: 0.9465 - label_output_loss: 0.6898 - val_loss: 1.0179 - val_decoded_loss: 0.3294 - val_label_output_loss: 0.6885\n",
            "Epoch 60/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6223 - decoded_loss: 0.9325 - label_output_loss: 0.6898 - val_loss: 1.0193 - val_decoded_loss: 0.3307 - val_label_output_loss: 0.6886\n",
            "Epoch 61/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6085 - decoded_loss: 0.9187 - label_output_loss: 0.6898 - val_loss: 1.0183 - val_decoded_loss: 0.3296 - val_label_output_loss: 0.6887\n",
            "Epoch 62/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6150 - decoded_loss: 0.9252 - label_output_loss: 0.6897 - val_loss: 1.0155 - val_decoded_loss: 0.3269 - val_label_output_loss: 0.6886\n",
            "Epoch 63/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5888 - decoded_loss: 0.8990 - label_output_loss: 0.6898 - val_loss: 1.0253 - val_decoded_loss: 0.3368 - val_label_output_loss: 0.6886\n",
            "Epoch 64/1000\n",
            "346/346 [==============================] - 15s 43ms/step - loss: 1.6003 - decoded_loss: 0.9105 - label_output_loss: 0.6898 - val_loss: 1.0239 - val_decoded_loss: 0.3354 - val_label_output_loss: 0.6885\n",
            "Epoch 65/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5965 - decoded_loss: 0.9067 - label_output_loss: 0.6898 - val_loss: 1.0226 - val_decoded_loss: 0.3341 - val_label_output_loss: 0.6884\n",
            "Epoch 66/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5819 - decoded_loss: 0.8922 - label_output_loss: 0.6897 - val_loss: 1.0269 - val_decoded_loss: 0.3382 - val_label_output_loss: 0.6887\n",
            "Epoch 67/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6258 - decoded_loss: 0.9360 - label_output_loss: 0.6898 - val_loss: 1.0184 - val_decoded_loss: 0.3298 - val_label_output_loss: 0.6886\n",
            "Epoch 68/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6007 - decoded_loss: 0.9110 - label_output_loss: 0.6897 - val_loss: 1.0128 - val_decoded_loss: 0.3245 - val_label_output_loss: 0.6884\n",
            "Epoch 69/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5793 - decoded_loss: 0.8895 - label_output_loss: 0.6898 - val_loss: 1.0183 - val_decoded_loss: 0.3299 - val_label_output_loss: 0.6884\n",
            "Epoch 70/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6000 - decoded_loss: 0.9102 - label_output_loss: 0.6898 - val_loss: 1.0260 - val_decoded_loss: 0.3373 - val_label_output_loss: 0.6888\n",
            "Epoch 71/1000\n",
            "346/346 [==============================] - 16s 47ms/step - loss: 1.6704 - decoded_loss: 0.9806 - label_output_loss: 0.6898 - val_loss: 1.0185 - val_decoded_loss: 0.3299 - val_label_output_loss: 0.6886\n",
            "Epoch 72/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6067 - decoded_loss: 0.9169 - label_output_loss: 0.6898 - val_loss: 1.0181 - val_decoded_loss: 0.3294 - val_label_output_loss: 0.6887\n",
            "Epoch 73/1000\n",
            "346/346 [==============================] - 15s 45ms/step - loss: 1.6057 - decoded_loss: 0.9159 - label_output_loss: 0.6898 - val_loss: 1.0160 - val_decoded_loss: 0.3275 - val_label_output_loss: 0.6886\n",
            "Epoch 74/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6045 - decoded_loss: 0.9147 - label_output_loss: 0.6898 - val_loss: 1.0201 - val_decoded_loss: 0.3315 - val_label_output_loss: 0.6885\n",
            "Epoch 75/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.6063 - decoded_loss: 0.9166 - label_output_loss: 0.6897 - val_loss: 1.0194 - val_decoded_loss: 0.3309 - val_label_output_loss: 0.6885\n",
            "Epoch 76/1000\n",
            "346/346 [==============================] - 15s 44ms/step - loss: 1.5927 - decoded_loss: 0.9030 - label_output_loss: 0.6898 - val_loss: 1.0175 - val_decoded_loss: 0.3288 - val_label_output_loss: 0.6887\n",
            "Epoch 77/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.6081 - decoded_loss: 0.9184 - label_output_loss: 0.6897 - val_loss: 1.0229 - val_decoded_loss: 0.3343 - val_label_output_loss: 0.6886\n",
            "Epoch 78/1000\n",
            "346/346 [==============================] - 16s 45ms/step - loss: 1.5909 - decoded_loss: 0.9012 - label_output_loss: 0.6897 - val_loss: 1.0287 - val_decoded_loss: 0.3400 - val_label_output_loss: 0.6886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG4ehgiRqPbd",
        "outputId": "9ac88cc0-96e1-40df-de99-2ddc1d94f740"
      },
      "source": [
        "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\r\n",
        "\r\n",
        "tuner = CVTuner(\r\n",
        "        hypermodel=model_fn,\r\n",
        "        oracle=kt.oracles.BayesianOptimization(\r\n",
        "        objective= kt.Objective('val_auc', direction='max'),\r\n",
        "        num_initial_points=4,\r\n",
        "        max_trials=20))\r\n",
        "\r\n",
        "FOLDS = 5\r\n",
        "SEED = 42\r\n",
        "\r\n",
        "if TRAINING:\r\n",
        "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\r\n",
        "    splits = list(gkf.split(y, groups=train['date'].values))\r\n",
        "    tuner.search((X,),(y,),splits=splits,batch_size=4096,epochs=100,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\r\n",
        "    hp  = tuner.get_best_hyperparameters(1)[0]\r\n",
        "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\r\n",
        "    for fold, (train_indices, test_indices) in enumerate(splits):\r\n",
        "        model = model_fn(hp)\r\n",
        "        X_train, X_test = X[train_indices], X[test_indices]\r\n",
        "        y_train, y_test = y[train_indices], y[test_indices]\r\n",
        "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\r\n",
        "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\r\n",
        "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\r\n",
        "        model.fit(X_test,y_test,epochs=3,batch_size=4096)\r\n",
        "        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\r\n",
        "    tuner.results_summary()\r\n",
        "# else:\r\n",
        "#     models = []\r\n",
        "#     hp = pd.read_pickle(f'../input/vae223/best_hp_{SEED}.pkl')\r\n",
        "#     for f in range(FOLDS):\r\n",
        "#         model = model_fn(hp)\r\n",
        "#         if USE_FINETUNE:\r\n",
        "#             model.load_weights(f'../input/vae223/model_{SEED}_{f}_finetune.hdf5')\r\n",
        "#         else:\r\n",
        "#             model.load_weights(f'../input/vae223/model_{SEED}_{f}.hdf5')\r\n",
        "#         models.append(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 6 Complete [00h 41m 49s]\n",
            "val_auc: 0.5413760304450989\n",
            "\n",
            "Best val_auc So Far: 0.5431180477142334\n",
            "Total elapsed time: 02h 34m 35s\n",
            "\n",
            "Search: Running Trial #7\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "init_dropout      |0.051911          |0.028706          \n",
            "num_layers        |1                 |3                 \n",
            "num_units_{i}     |241               |236               \n",
            "dropout_0         |0.050662          |0.073568          \n",
            "lr                |0.0044609         |0.0025945         \n",
            "label_smoothing   |0.011061          |0.088484          \n",
            "dropout_1         |0.10738           |0.46779           \n",
            "dropout_2         |0.34843           |0.32294           \n",
            "\n",
            "Epoch 1/100\n",
            "36/36 [==============================] - 9s 221ms/step - loss: 0.7197 - auc: 0.5200 - val_loss: 0.7091 - val_auc: 0.5262\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 7s 210ms/step - loss: 0.6896 - auc: 0.5457 - val_loss: 0.6996 - val_auc: 0.5336\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 7s 208ms/step - loss: 0.6871 - auc: 0.5552 - val_loss: 0.6969 - val_auc: 0.5318\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 7s 205ms/step - loss: 0.6854 - auc: 0.5604 - val_loss: 0.6953 - val_auc: 0.5324\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.6843 - auc: 0.5632 - val_loss: 0.6970 - val_auc: 0.5320\n",
            "Epoch 1/100\n",
            "91/91 [==============================] - 16s 161ms/step - loss: 0.7076 - auc: 0.5224 - val_loss: 0.6939 - val_auc: 0.5371\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 14s 159ms/step - loss: 0.6893 - auc: 0.5457 - val_loss: 0.6926 - val_auc: 0.5356\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 14s 159ms/step - loss: 0.6881 - auc: 0.5504 - val_loss: 0.6912 - val_auc: 0.5370\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 14s 158ms/step - loss: 0.6872 - auc: 0.5541 - val_loss: 0.6916 - val_auc: 0.5364\n",
            "Epoch 1/100\n",
            "152/152 [==============================] - 24s 148ms/step - loss: 0.7035 - auc: 0.5274 - val_loss: 0.6920 - val_auc: 0.5400\n",
            "Epoch 2/100\n",
            "152/152 [==============================] - 22s 144ms/step - loss: 0.6893 - auc: 0.5461 - val_loss: 0.6903 - val_auc: 0.5424\n",
            "Epoch 3/100\n",
            "152/152 [==============================] - 22s 143ms/step - loss: 0.6883 - auc: 0.5504 - val_loss: 0.6909 - val_auc: 0.5398\n",
            "Epoch 4/100\n",
            "152/152 [==============================] - 22s 145ms/step - loss: 0.6880 - auc: 0.5512 - val_loss: 0.6902 - val_auc: 0.5433\n",
            "Epoch 5/100\n",
            "152/152 [==============================] - 22s 144ms/step - loss: 0.6875 - auc: 0.5537 - val_loss: 0.6906 - val_auc: 0.5420\n",
            "Epoch 6/100\n",
            "152/152 [==============================] - 22s 146ms/step - loss: 0.6871 - auc: 0.5553 - val_loss: 0.6906 - val_auc: 0.5403\n",
            "Epoch 7/100\n",
            "152/152 [==============================] - 22s 145ms/step - loss: 0.6868 - auc: 0.5564 - val_loss: 0.6905 - val_auc: 0.5432\n",
            "Epoch 1/100\n",
            "218/218 [==============================] - 31s 139ms/step - loss: 0.7012 - auc: 0.5284 - val_loss: 0.6910 - val_auc: 0.5370\n",
            "Epoch 2/100\n",
            "218/218 [==============================] - 30s 137ms/step - loss: 0.6893 - auc: 0.5457 - val_loss: 0.6907 - val_auc: 0.5391\n",
            "Epoch 3/100\n",
            "218/218 [==============================] - 30s 137ms/step - loss: 0.6885 - auc: 0.5491 - val_loss: 0.6906 - val_auc: 0.5399\n",
            "Epoch 4/100\n",
            "218/218 [==============================] - 31s 141ms/step - loss: 0.6879 - auc: 0.5524 - val_loss: 0.6911 - val_auc: 0.5374\n",
            "Epoch 5/100\n",
            "218/218 [==============================] - 30s 137ms/step - loss: 0.6876 - auc: 0.5533 - val_loss: 0.6915 - val_auc: 0.5381\n",
            "Epoch 6/100\n",
            "218/218 [==============================] - 30s 137ms/step - loss: 0.6875 - auc: 0.5537 - val_loss: 0.6910 - val_auc: 0.5402\n",
            "Epoch 7/100\n",
            "218/218 [==============================] - 30s 139ms/step - loss: 0.6872 - auc: 0.5550 - val_loss: 0.6914 - val_auc: 0.5379\n",
            "Epoch 8/100\n",
            "218/218 [==============================] - 30s 138ms/step - loss: 0.6868 - auc: 0.5558 - val_loss: 0.6913 - val_auc: 0.5398\n",
            "Epoch 9/100\n",
            "218/218 [==============================] - 30s 136ms/step - loss: 0.6866 - auc: 0.5572 - val_loss: 0.6917 - val_auc: 0.5385\n",
            "Epoch 1/100\n",
            "287/287 [==============================] - 40s 135ms/step - loss: 0.6997 - auc: 0.5286 - val_loss: 0.6895 - val_auc: 0.5449\n",
            "Epoch 2/100\n",
            "287/287 [==============================] - 38s 134ms/step - loss: 0.6895 - auc: 0.5446 - val_loss: 0.6888 - val_auc: 0.5476\n",
            "Epoch 3/100\n",
            "287/287 [==============================] - 38s 134ms/step - loss: 0.6888 - auc: 0.5480 - val_loss: 0.6890 - val_auc: 0.5471\n",
            "Epoch 4/100\n",
            "287/287 [==============================] - 38s 134ms/step - loss: 0.6886 - auc: 0.5489 - val_loss: 0.6889 - val_auc: 0.5470\n",
            "Epoch 5/100\n",
            "287/287 [==============================] - 39s 136ms/step - loss: 0.6882 - auc: 0.5507 - val_loss: 0.6883 - val_auc: 0.5507\n",
            "Epoch 6/100\n",
            "287/287 [==============================] - 38s 134ms/step - loss: 0.6880 - auc: 0.5519 - val_loss: 0.6882 - val_auc: 0.5511\n",
            "Epoch 7/100\n",
            "287/287 [==============================] - 39s 135ms/step - loss: 0.6877 - auc: 0.5528 - val_loss: 0.6885 - val_auc: 0.5502\n",
            "Epoch 8/100\n",
            "287/287 [==============================] - 39s 135ms/step - loss: 0.6876 - auc: 0.5533 - val_loss: 0.6885 - val_auc: 0.5511\n",
            "Epoch 9/100\n",
            "212/287 [=====================>........] - ETA: 9s - loss: 0.6873 - auc: 0.5549Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF749I58qPZB"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/Kaggle\r\n",
        "!ls\r\n",
        "if not  TRAINING:\r\n",
        "    f = np.median\r\n",
        "    models = models[-2:]\r\n",
        "    import janestreet\r\n",
        "    env = janestreet.make_env()\r\n",
        "    th = 0.5\r\n",
        "    for (test_df, pred_df) in tqdm(env.iter_test()):\r\n",
        "        if test_df['weight'].item() > 0:\r\n",
        "            x_tt = test_df.loc[:, features].values\r\n",
        "            if np.isnan(x_tt[:, 1:].sum()):\r\n",
        "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\r\n",
        "            pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\r\n",
        "            pred = f(pred)\r\n",
        "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\r\n",
        "        else:\r\n",
        "            pred_df.action = 0\r\n",
        "        env.predict(pred_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXpby0OyqPWE"
      },
      "source": [
        " %cd /content/gdrive/MyDrive/Kaggle/janestreet/\r\n",
        "# !cp competition.cpython-37m-x86_64-linux-gnu.so /content/gdrive/MyDrive/Kaggle/janestreet/\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAAU6Xm-qPNG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}